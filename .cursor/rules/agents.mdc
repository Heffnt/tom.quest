---
description: Project context for TOM.quest - UPDATE THIS FILE after any changes or fixes
alwaysApply: true
---

# TOM.quest

**AGENT REQUIREMENT: After ANY code change, add/update relevant info here. After ANY bug fix, add to Fixes section. This prevents repeated mistakes and keeps context accurate.**

## Stack
Next.js 16 (App Router) | TypeScript | Tailwind v4 (PostCSS) | Geist fonts | Vercel

## Design
- Colors: black bg, white text (60%/40% opacity for secondary)
- Layout: max-w-3xl mx-auto, px-6 py-16, min-h-screen
- Animations: animate-fade-in, animate-fade-in-delay (globals.css)

## Structure
```
app/                      # Next.js frontend
├── turing/page.tsx       # GPU dashboard (client component)
├── api/turing/           # Proxy routes → Turing backend
│   ├── gpu-report/       # GET gpu availability
│   ├── allocate/         # POST allocation request
│   ├── jobs/             # GET list, DELETE cancel
│   └── dirs/             # GET directory listing
└── components/Navigation.tsx

tom-quest-api/            # FastAPI on Turing HPC
├── main.py, gpu_report.py, slurm.py, screens.py, dirs.py, job_screens.py
```

## Turing Dashboard
- Proxy: frontend → /api/turing/* → Turing backend (via Cloudflare quick tunnel)
- URL discovery: API auto-updates GitHub Gist → Vercel reads from `TURING_URL_GIST` (1min cache TTL)
- After API restart: expect ~1-2min of 530 errors while tunnel establishes + caches refresh
- Auth: X-API-Key header (TURING_API_KEY on Vercel, API_KEY on Turing)
- Turing .env: API_KEY, GITHUB_TOKEN (gist scope), GIST_ID
- Vercel env: TURING_URL_GIST (raw gist url), TURING_API_KEY
- GPUs: nvidia=H100, tesla=V100, max 12 allocations
- GPU Report API: returns `{nodes: NodeInfo[], summary: {free, available, unavailable}, notes}`
  - NodeInfo: `{name, gpu_type, total_gpus, allocated_gpus, state, memory_total_mb, memory_allocated_mb}`
  - state: "up" | "down" | "drain"
- GPU Visual Grid: SVG boxes per GPU (green=free, gray=in use, red=down), memory bar per node
- Allocation: --job-name=tom.quest, non-blocking salloc, default 1440 mins
- Screens: `{project_name}_{index}` (1-indexed), no -dm flag, polls until RUNNING then creates screen + srun + commands
- Screen mapping: `~/.tom-quest-screens.json` stores job_id → screen_name (jobs without mapping show "-")
- localStorage: `turing_project_commands`, `turing_auto_refresh`, `turing_refresh_interval`
- Auto-refresh: unified at top for all panels, configurable interval (default 30s), each panel has own refresh button
- Job status: shows squeue reason in parens (e.g. "PENDING (Resources)", "RUNNING (gpu-21)")
- Dir browse: always sends path (default `~`), surfaces upstream error
- Turing setup: clone repo to `~/tom.quest`, use `tom-quest-api/` subdir

## Fixes & Lessons
<!-- Add entries: [date] issue → fix -->
- [2026-01-28] /api/turing/dirs 404 without path → default to `~`, show upstream error
- [2026-01-29] Cloudflare 524/HTML responses → auto-start tunnel + file logs in API
- [2026-01-29] salloc timeout on resource wait → non-blocking Popen, capture job ID from early output, screen polls for RUNNING before commands
- [2026-01-29] Active Jobs showed wrong screen name → added job_screens.py mapping file, simplified screen names to `{project}_{index}`, external jobs show "-"
- [2026-01-29] 530 errors after API restart → transient: wait ~1-2min for new tunnel + gist update + Vercel cache (1min TTL) to refresh
- [2026-01-29] srun failed on PENDING job → poll for RUNNING first, then create screen and send srun
- [2026-01-29] Screen names 1-indexed, job status shows squeue reason, unified auto-refresh controls at top
- [2026-01-29] Debug terminal: resizable (drag header), dynamic bottom padding, copy button, fixed hover transparency
- [2026-01-29] GPU Availability: visual SVG grid with per-node GPU boxes + memory bars, API returns per-node data instead of aggregates

## Placeholders
- /projects: showcase work
- /data-labeling: annotation tools
